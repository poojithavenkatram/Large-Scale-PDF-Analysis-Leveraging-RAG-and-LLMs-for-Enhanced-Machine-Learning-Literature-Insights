{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral 7B + FAISS using Langchain\n",
        "\n",
        "#### Poojitha Venkatram"
      ],
      "metadata": {
        "id": "HzJndzSe-r8c"
      },
      "id": "HzJndzSe-r8c"
    },
    {
      "cell_type": "markdown",
      "id": "d67a4729-cd2f-47e7-a4f6-f84a5677414f",
      "metadata": {
        "id": "d67a4729-cd2f-47e7-a4f6-f84a5677414f"
      },
      "source": [
        "# Basic RAG\n",
        "Retrieval-augmented generation (RAG) is an AI framework that synergizes the capabilities of LLMs and information retrieval systems. It’s useful to answer questions or generate content leveraging external knowledge. There are two main steps in RAG: 1) retrieval: retrieve relevant information from a knowledge base with text embeddings stored in a vectore store; 2) generation: insert the relevant information to the prompt for the LLM to generate information.\n",
        "\n",
        "\n",
        "\n",
        "### Import all the needed packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b880d1ed-3db0-45a1-807e-1b47e9ce1320",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b880d1ed-3db0-45a1-807e-1b47e9ce1320",
        "outputId": "634b2daa-2524-43cb-bb55-12b3099a655f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Collecting mistralai==0.0.12\n",
            "  Using cached mistralai-0.0.12-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: httpx<0.26.0,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from mistralai==0.0.12) (0.25.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.10 in /usr/local/lib/python3.10/dist-packages (from mistralai==0.0.12) (3.10.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from mistralai==0.0.12) (2.6.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.0.12) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.0.12) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.0.12) (4.11.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (1.2.0)\n",
            "Installing collected packages: mistralai\n",
            "  Attempting uninstall: mistralai\n",
            "    Found existing installation: mistralai 0.0.11\n",
            "    Uninstalling mistralai-0.0.11:\n",
            "      Successfully uninstalled mistralai-0.0.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-mistralai 0.0.4 requires mistralai<0.0.12,>=0.0.11, but you have mistralai 0.0.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mistralai-0.0.12\n"
          ]
        }
      ],
      "source": [
        "! pip install faiss-cpu==1.7.4 mistralai==0.0.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKPJ3PMvHEOc",
        "outputId": "506dc432-2ed0-4b4b-b434-1a9d799adac5"
      },
      "id": "UKPJ3PMvHEOc",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZczgC3aVHIPn",
        "outputId": "e391443b-e3f3-471b-cfd1-6e302a86113c"
      },
      "id": "ZczgC3aVHIPn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Mistral-7B-Instruct-v0.2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Mistral-7B-Instruct-v0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY4a39sdHiW4",
        "outputId": "cdfffdcd-c47d-423d-bf79-25bba54ed50e"
      },
      "id": "HY4a39sdHiW4",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mistral-7B-Instruct-v0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b83adf94-9059-4779-afcb-2d273a66c695",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b83adf94-9059-4779-afcb-2d273a66c695",
        "outputId": "5f052db0-cb2e-49ea-e466-469ade4e783e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: langchain-mistralai==0.0.4 in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai==0.0.4) (0.1.42)\n",
            "Collecting mistralai<0.0.12,>=0.0.11 (from langchain-mistralai==0.0.4)\n",
            "  Using cached mistralai-0.0.11-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai==0.0.4) (0.15.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.32)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.47)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-mistralai==0.0.4) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: httpx<0.26.0,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from mistralai<0.0.12,>=0.0.11->langchain-mistralai==0.0.4) (0.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.16.0,>=0.15.1->langchain-mistralai==0.0.4) (0.20.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai<0.0.12,>=0.0.11->langchain-mistralai==0.0.4) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai<0.0.12,>=0.0.11->langchain-mistralai==0.0.4) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai<0.0.12,>=0.0.11->langchain-mistralai==0.0.4) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.26.0,>=0.25.2->mistralai<0.0.12,>=0.0.11->langchain-mistralai==0.0.4) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->langchain-mistralai==0.0.4) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->langchain-mistralai==0.0.4) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->langchain-mistralai==0.0.4) (4.66.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.26.0,>=0.25.2->mistralai<0.0.12,>=0.0.11->langchain-mistralai==0.0.4) (1.2.0)\n",
            "Installing collected packages: mistralai\n",
            "  Attempting uninstall: mistralai\n",
            "    Found existing installation: mistralai 0.0.12\n",
            "    Uninstalling mistralai-0.0.12:\n",
            "      Successfully uninstalled mistralai-0.0.12\n",
            "Successfully installed mistralai-0.0.11\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-mistralai==0.0.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrtbKg1Y0BNi",
        "outputId": "7ee75843-54ac-44f1-ac88-60016e37ab22"
      },
      "id": "DrtbKg1Y0BNi",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install 'huggingface_hub[cli,torch]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbAuNYlW2s1_",
        "outputId": "60a89a41-d9b0-4dbf-99ab-a214bca6ac5f"
      },
      "id": "pbAuNYlW2s1_",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli,torch] in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (23.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli,torch]) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli,torch]) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli,torch]) (3.0.43)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli,torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli,torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli,torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli,torch]) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->huggingface_hub[cli,torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->huggingface_hub[cli,torch]) (12.4.127)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli,torch]) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->huggingface_hub[cli,torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->huggingface_hub[cli,torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
      ],
      "metadata": {
        "id": "wFtX0fp61XP0"
      },
      "id": "wFtX0fp61XP0",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "4ec5e8da894e441da7f5d4571247a454",
            "79d3cbc6539247469b1f8123d69fcc46",
            "c65087462b1f4266b01553b9d97b10b1",
            "03ddf48c96dc4aa39dd08bc57baeff84",
            "bd4df7400d7042f08dce74afe7bf0794",
            "56de2393a5634d67a4aaf59643ce0789",
            "ff0a419a0d094129b750947f4d2e0229",
            "138b5dc066944f03af93a6f396ab1e35",
            "8be2a5ee0e314ba48cc07882696c0374",
            "4f965ccbf6a8414ea16b3a31ebe7aede",
            "6daa5eee8f974d26982b671b1ee3c11e"
          ]
        },
        "id": "4Jsy2xD03bz8",
        "outputId": "4328b465-b596-4001-e37b-f50837571058"
      },
      "id": "4Jsy2xD03bz8",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ec5e8da894e441da7f5d4571247a454"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe8609d5-9f27-4202-b0be-36db34412998",
      "metadata": {
        "id": "fe8609d5-9f27-4202-b0be-36db34412998"
      },
      "source": [
        "### Get data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-htGtGiMcC5",
        "outputId": "50b38714-65f7-4611-944f-0df869ed9cda"
      },
      "id": "u-htGtGiMcC5",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubnpfx_2OzpT",
        "outputId": "259a3c07-e426-4b3b-b648-770433774d92"
      },
      "id": "Ubnpfx_2OzpT",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c4c01740-72b4-482c-b61e-e272a734f01f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4c01740-72b4-482c-b61e-e272a734f01f",
        "outputId": "5b3a8b43-ac95-4bd1-9e7f-7402d9948f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Prediction of Structural System Subject to  Earthquake \n",
            "Motions using Artificial Neural Network \n",
            " \n",
            "S. Chakraverty*,  T. Marwala** , Pallavi Gupta* and  Thando Tettey**  \n",
            " \n",
            "*B.P.P.P. Division, Central Building Research Institu te \n",
            "Roorkee-247 667, Uttaranchal, India \n",
            "e-mail :sne_chak@yahoo.com \n",
            " \n",
            "** School of Electrical and Information Engineering, \n",
            "University of the Witwatersrand, Private Bag 3 \n",
            "Wits, 2050,Republic of South Africa  \n",
            "  \n",
            "Abstract \n",
            "This paper uses Artificial Neural Network\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file\n",
        "with open(\"/content/drive/MyDrive/Sample.pdf\", \"rb\") as file:\n",
        "    pdf = PyPDF2.PdfReader(file)\n",
        "\n",
        "    # Initialize a variable to hold all text\n",
        "    full_text = \"\"\n",
        "\n",
        "    # Iterate through each page and extract text\n",
        "    for page in pdf.pages:\n",
        "        full_text += page.extract_text()\n",
        "\n",
        "# Printing the extracted text first 500 characters to check the extraction\n",
        "print(full_text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15af760a-f074-4339-81f9-4f370034355e",
      "metadata": {
        "id": "15af760a-f074-4339-81f9-4f370034355e"
      },
      "source": [
        "We can also save the essay in a local file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f03f47af-a20b-4122-a114-74b9748ff543",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f03f47af-a20b-4122-a114-74b9748ff543",
        "outputId": "83936133-d296-4efa-f992-bde211a457ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24311"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad1aa61-9e1c-46c8-ae5e-61855df440f9",
      "metadata": {
        "id": "aad1aa61-9e1c-46c8-ae5e-61855df440f9"
      },
      "source": [
        "## Split document into chunks\n",
        "\n",
        "In a RAG system, it is crucial to split the document into smaller chunks so that it’s more effective to identify and retrieve the most relevant information in the retrieval process later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8494655e-bd87-49de-8f1d-69ffbc1c256e",
      "metadata": {
        "id": "8494655e-bd87-49de-8f1d-69ffbc1c256e"
      },
      "outputs": [],
      "source": [
        "chunk_size = 2048\n",
        "chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c78c9936-0c1d-471c-b030-6c45639e7238",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c78c9936-0c1d-471c-b030-6c45639e7238",
        "outputId": "807af48f-62eb-440e-9bf4-2f746f33abcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e77d9805-7a53-4210-9f80-f4de52285588",
      "metadata": {
        "id": "e77d9805-7a53-4210-9f80-f4de52285588"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def get_text_embedding(text):\n",
        "    # Tokenize the input text and convert to input tensors\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**tokens)\n",
        "\n",
        "    # You might want to take the mean of the last hidden state as the embedding\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1).numpy()\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
      ],
      "metadata": {
        "id": "inZW3SsmY2NR"
      },
      "id": "inZW3SsmY2NR",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ca875993-fe6d-42df-811e-a43891cd0350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca875993-fe6d-42df-811e-a43891cd0350",
        "outputId": "5c430e58-7bb4-4a0a-86c1-974028975826"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 1, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "text_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "55396758-c3f3-45b3-b6e7-d4912c0899f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55396758-c3f3-45b3-b6e7-d4912c0899f2",
        "outputId": "7e74fe1d-be7a-48fa-b530-4ff207ecfd46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.17309807, -0.10922703,  0.11997147, ...,  0.11830396,\n",
              "         -0.079504  , -0.11430095]],\n",
              "\n",
              "       [[-0.18128201, -0.14044508,  0.1545095 , ...,  0.10432599,\n",
              "          0.00846069, -0.04908906]],\n",
              "\n",
              "       [[-0.20292331, -0.10011439,  0.12901923, ...,  0.08570072,\n",
              "         -0.02853273, -0.10067203]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-0.17451054, -0.00692823,  0.17774832, ...,  0.04262985,\n",
              "         -0.02326536, -0.09262711]],\n",
              "\n",
              "       [[-0.03157317, -0.13334677,  0.07569563, ...,  0.01338292,\n",
              "         -0.08685192, -0.1011356 ]],\n",
              "\n",
              "       [[-0.03780447, -0.08106704,  0.04931236, ...,  0.01181052,\n",
              "         -0.11106861, -0.05282832]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "text_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_text_embedding(text):\n",
        "    # This should return a real embedding based on your model\n",
        "    return np.random.rand(384)\n",
        "\n",
        "# Example list of texts\n",
        "texts = [\"text1\", \"text2\", \"text3\"]\n",
        "\n",
        "# Generate embeddings for each text\n",
        "text_embeddings = np.array([get_text_embedding(text) for text in texts])\n",
        "\n",
        "print(\"Generated embeddings shape:\", text_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWw5MZ-GR3rx",
        "outputId": "6df55089-53d9-4211-b51d-ac48292e46a0"
      },
      "id": "iWw5MZ-GR3rx",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings shape: (3, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "# Assuming text_embeddings is your array with shape (3, 384)\n",
        "d = text_embeddings.shape[1]  # Dimension of the embeddings (should be 384)\n",
        "\n",
        "# Creating the FAISS index\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Adding the embeddings to the index\n",
        "index.add(text_embeddings)"
      ],
      "metadata": {
        "id": "oHo0zGtxSP3T"
      },
      "id": "oHo0zGtxSP3T",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1cba33c7-9d1d-44d8-a01e-e30f16be1aac",
      "metadata": {
        "id": "1cba33c7-9d1d-44d8-a01e-e30f16be1aac"
      },
      "source": [
        "### Load into a vector database\n",
        "Once we get the text embeddings, a common practice is to store them in a vector database for efficient processing and retrieval. Using an open-source vector database Faiss, which allows for efficient similarity search.  \n",
        "\n",
        "With Faiss, we instantiate an instance of the Index class, which defines the indexing structure of the vector database. We then add the text embeddings to this indexing structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6a5b1877-b113-4527-9055-cae9049fef08",
      "metadata": {
        "id": "6a5b1877-b113-4527-9055-cae9049fef08"
      },
      "outputs": [],
      "source": [
        "d = text_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(text_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee023ab-b26c-4df5-8a7b-7dd660bfad86",
      "metadata": {
        "id": "5ee023ab-b26c-4df5-8a7b-7dd660bfad86"
      },
      "source": [
        "\n",
        "\n",
        "### Create embeddings for a question\n",
        "Whenever users ask a question, we also need to create embeddings for this question using the same embedding models as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "894d9764-9da9-4629-8f2a-c9dcaf6ceb8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "894d9764-9da9-4629-8f2a-c9dcaf6ceb8d",
        "outputId": "94140b8f-5c7a-493a-9be5-99850cbb564f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "question = \"How to write an abstract similar to the response prediction using ANN paper?\"\n",
        "question_embeddings = np.array([get_text_embedding(question)])\n",
        "question_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9c4948cc-6d8b-449f-bc00-abb3591c7222",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c4948cc-6d8b-449f-bc00-abb3591c7222",
        "outputId": "4a0382cc-6b3d-4dd4-aa7f-f4c326f97e3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.87802083, 0.36368342, 0.62105168, 0.72783167, 0.64696208,\n",
              "        0.2486663 , 0.75119849, 0.38147891, 0.20406318, 0.75687456,\n",
              "        0.87696904, 0.85323777, 0.57878798, 0.22556383, 0.62671107,\n",
              "        0.77638152, 0.55871051, 0.9421258 , 0.54221419, 0.19933051,\n",
              "        0.27942118, 0.67654684, 0.86496604, 0.78607431, 0.61996087,\n",
              "        0.38345325, 0.01384411, 0.36641655, 0.54616124, 0.46914813,\n",
              "        0.09803941, 0.57252951, 0.38135483, 0.50120672, 0.85621412,\n",
              "        0.22171292, 0.17454722, 0.49837733, 0.58871526, 0.68125253,\n",
              "        0.71555964, 0.93322098, 0.83178476, 0.76875748, 0.63129956,\n",
              "        0.68690865, 0.53978664, 0.04060748, 0.61009988, 0.96331778,\n",
              "        0.81801412, 0.76830599, 0.41979903, 0.52545301, 0.27009569,\n",
              "        0.93573108, 0.27216655, 0.25312146, 0.7029919 , 0.51631519,\n",
              "        0.12121192, 0.00418767, 0.32248962, 0.33006628, 0.96600258,\n",
              "        0.46138347, 0.57217572, 0.18556137, 0.09911915, 0.32190901,\n",
              "        0.4799523 , 0.55010289, 0.50235695, 0.80435274, 0.2016787 ,\n",
              "        0.89107538, 0.34329681, 0.01018799, 0.18440801, 0.34452251,\n",
              "        0.33607156, 0.22951679, 0.06593349, 0.8767576 , 0.33551867,\n",
              "        0.48388535, 0.88361843, 0.94158409, 0.02504212, 0.87393498,\n",
              "        0.89014588, 0.2260172 , 0.61358685, 0.34998573, 0.90695418,\n",
              "        0.00438179, 0.61848037, 0.6645497 , 0.44573744, 0.50334256,\n",
              "        0.17306074, 0.31747198, 0.05779783, 0.91246762, 0.82649629,\n",
              "        0.33236125, 0.86850791, 0.53093488, 0.12971947, 0.91328707,\n",
              "        0.43912241, 0.38029837, 0.78087201, 0.96577969, 0.82478932,\n",
              "        0.62960315, 0.29512347, 0.49704675, 0.462638  , 0.66210842,\n",
              "        0.11084047, 0.62633361, 0.7013685 , 0.77765664, 0.13279186,\n",
              "        0.31846489, 0.15920415, 0.99382449, 0.7086358 , 0.88344531,\n",
              "        0.2359641 , 0.06942467, 0.13260047, 0.17979722, 0.93848282,\n",
              "        0.95398559, 0.50245979, 0.38333999, 0.06165716, 0.40655019,\n",
              "        0.36064214, 0.94632067, 0.50711669, 0.40018993, 0.74067152,\n",
              "        0.4989242 , 0.46992273, 0.12625987, 0.75407189, 0.20260149,\n",
              "        0.7705322 , 0.66079397, 0.96538782, 0.39006277, 0.9761173 ,\n",
              "        0.1157976 , 0.12079877, 0.68122141, 0.96316568, 0.67765116,\n",
              "        0.51955482, 0.29634619, 0.35187095, 0.45867604, 0.40241263,\n",
              "        0.57637994, 0.81101745, 0.19941003, 0.74872358, 0.38219757,\n",
              "        0.19022065, 0.28925506, 0.64139477, 0.76789362, 0.01288504,\n",
              "        0.20400235, 0.50782179, 0.55721236, 0.38603963, 0.3607651 ,\n",
              "        0.27067635, 0.73342648, 0.17038334, 0.14300679, 0.81823114,\n",
              "        0.65922384, 0.0921179 , 0.51996461, 0.23996584, 0.21653631,\n",
              "        0.00692293, 0.60223235, 0.51321066, 0.95983712, 0.80902768,\n",
              "        0.8924028 , 0.35541355, 0.16147087, 0.38329588, 0.53196811,\n",
              "        0.79667596, 0.89645856, 0.53996611, 0.94038218, 0.47614655,\n",
              "        0.9987779 , 0.44432499, 0.63802675, 0.66253932, 0.99420013,\n",
              "        0.58462002, 0.83527192, 0.01032695, 0.45374858, 0.37857394,\n",
              "        0.32138454, 0.17169631, 0.74125253, 0.71724565, 0.26158274,\n",
              "        0.97041254, 0.44969249, 0.17588569, 0.20989033, 0.67533392,\n",
              "        0.76890284, 0.68377763, 0.34157015, 0.19563421, 0.98979918,\n",
              "        0.04555942, 0.12707989, 0.94632045, 0.06747763, 0.0746829 ,\n",
              "        0.70205182, 0.7030106 , 0.5015895 , 0.42795235, 0.47604962,\n",
              "        0.97905237, 0.31263164, 0.75478961, 0.34718957, 0.63275281,\n",
              "        0.50200078, 0.7897876 , 0.80704158, 0.52970411, 0.26827594,\n",
              "        0.0886885 , 0.57717408, 0.64386618, 0.99256953, 0.8327537 ,\n",
              "        0.87981213, 0.00589792, 0.50456764, 0.8560489 , 0.28371032,\n",
              "        0.25644677, 0.62660073, 0.28512727, 0.69542383, 0.43055598,\n",
              "        0.23959288, 0.03724801, 0.84154856, 0.07044107, 0.00732283,\n",
              "        0.36529075, 0.88771308, 0.96332791, 0.74815822, 0.29475094,\n",
              "        0.90239231, 0.82107695, 0.7395689 , 0.84650208, 0.04960741,\n",
              "        0.57546677, 0.91624268, 0.56315778, 0.92599222, 0.11452298,\n",
              "        0.00909526, 0.577536  , 0.30386072, 0.43901379, 0.44707943,\n",
              "        0.89161406, 0.51276045, 0.66616315, 0.64986781, 0.56938848,\n",
              "        0.96547674, 0.60334498, 0.77842067, 0.04310596, 0.81361642,\n",
              "        0.40485607, 0.67546782, 0.15937057, 0.83037973, 0.23514681,\n",
              "        0.23596968, 0.29519745, 0.64092633, 0.32728895, 0.87472881,\n",
              "        0.65427919, 0.34517399, 0.28257111, 0.26908591, 0.92039738,\n",
              "        0.90507874, 0.05907927, 0.79360325, 0.40466289, 0.71456833,\n",
              "        0.52698789, 0.3956593 , 0.69005356, 0.98856037, 0.37816666,\n",
              "        0.0407882 , 0.90798179, 0.73931983, 0.54492579, 0.38538805,\n",
              "        0.41436386, 0.40772402, 0.5414443 , 0.53800978, 0.71622883,\n",
              "        0.70900333, 0.98940864, 0.2589132 , 0.36111441, 0.15814811,\n",
              "        0.45530081, 0.34186367, 0.44238314, 0.58062597, 0.24614284,\n",
              "        0.99179777, 0.92359285, 0.5166949 , 0.44445433, 0.31304454,\n",
              "        0.63232073, 0.85914627, 0.67472273, 0.25924381, 0.85834617,\n",
              "        0.72743828, 0.34386186, 0.28030344, 0.11083961, 0.71275527,\n",
              "        0.88967169, 0.99098486, 0.80468243, 0.43207111, 0.72932127,\n",
              "        0.73580875, 0.68742886, 0.27214145, 0.33898509, 0.84306312,\n",
              "        0.96314469, 0.17934968, 0.53321934, 0.17732957, 0.27009657,\n",
              "        0.35772315, 0.99671656, 0.9971436 , 0.69451746, 0.40924829,\n",
              "        0.19723311, 0.41278103, 0.86946753, 0.19078297]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "question_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15989e10-d0ec-41be-b6be-fa317565a926",
      "metadata": {
        "id": "15989e10-d0ec-41be-b6be-fa317565a926"
      },
      "source": [
        "\n",
        "\n",
        "### Retrieve similar chunks from the vector database\n",
        "We can perform a search on the vector database with `index.search`, which takes two arguments: the first is the vector of the question embeddings, and the second is the number of similar vectors to retrieve. This function returns the distances and the indices of the most similar vectors to the question vector in the vector database. Then based on the returned indices, we can retrieve the actual relevant text chunks that correspond to those indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c930b378-7aac-434c-881b-ab69d3edb93d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c930b378-7aac-434c-881b-ab69d3edb93d",
        "outputId": "f965e1b0-1da1-4ef9-e553-b7f3b09b2231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 1]]\n"
          ]
        }
      ],
      "source": [
        "D, I = index.search(question_embeddings, k=2)\n",
        "print(I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "73aab584-1dbf-4532-b41e-0403eeeeb567",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73aab584-1dbf-4532-b41e-0403eeeeb567",
        "outputId": "3e0deead-9691-4110-a887-fdda5b1e651d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"tures. Muhammad [13] gives certain ANN applications in concrete \\nstructures. Pandey and Barai [14] detected damage in a bridge truss by applying \\nANN of multilayer perceptron architectures to numericall y simulated data. Some \\nstudies such as [15]-[17] used artificial neural network f or structural damage \\ndetection and system identification.  \\n \\nIn the present paper, the Chamoli earthquake ground acceleration at Barkot (NE) \\nand Uttarkashi earthquake ground acceleration recorded a t Barkot (NE and NW)  \\nhave been considered based on the authors' previous stud y [18]. From their \\nground acceleration the responses are computed using the u sual procedure. \\nThen the ground acceleration and the corresponding re sponse are trained using \\nArtificial Neural Network (ANN) with and without damp ing. After training the \\nnetwork with one earthquake, the converged weight mat rices are stored. In order \\nto show the power of these converged (trained) networ ks other earthquakes are \\nused as input to predict the direct response of the structu re without using any \\nmathematical analysis of the response prediction. Simi larly, the various time \\nperiods of one earthquake and its corresponding maximu m responses are \\ntrained. Then the converged weights are used to predict  the maximum response \\ndirectly to the corresponding time period. Various othe r results related to use of \\nthese trained networks are discussed for future / other e arthquakes. \\n \\n2 Artificial Neural Network  \\n \\nArtificial neural systems are present day machines that ha ve great potential to \\nimprove the quality of our life. Advances have been made in applying such \\nsystems for problems found difficult for traditional co mputation. A neural network \\nis a parallel, distributed information processing structur e consists of processing \\nelements called neurons, which are interconnected and u nidirectional signal channels called connections. The general structure of the network that have \\nbeen used here is given in Fig.1. the structure consists of  three layers : the \", 'd esign of the building. All \\nbuildings have a \"natural frequency\" associated with the m. If strain is placed on \\nto the structure and then let it snap back into equilibr ium, it will sway back and \\nforth with an amplitude that decays with time. If the  ground shakes with the same \\nfrequency as a building\\'s natural frequency, it will cau se the amplitude of sway to \\nget larger and larger such that, the ground shaking is i n resonance with the \\nbuilding\\'s natural frequency. This produces the most stra in on the components of \\nthe building and can quickly cause the building to colla pse. Powerful technique of \\nArtificial Neural Network (ANN) has been used to model the problem for one \\nstorey structure. Among the different types of ANN, the  feedforward, multilayer, \\nsupervised neural network with error back propagation al gorithm, the BPN [1] is \\nthe most frequently applied NN model. Dynamic response of a structure to strong \\nearthquake ground motion may be investigated by diff erent methods. The \\nmethod, that has been used here, is to create a trained  black box containing the \\ncharacteristics of the structure and of the earthquake moti on which can predict \\nthe dynamic response for any other earthquake for a pa rticular structure.  \\n \\nArtificial Neural Network (ANN) have gradually been es tablished as a powerful \\nsoft computing tool in various fields because of their excellent learning capacity \\nand their high tolerance to partially inaccurate data.  ANN has, recently been \\napplied to assess damage in structures. Stefano et al.[3]  used probabilistic \\nNeural Networks for seismic damage prediction. Many meth ods viz. [4]-[9] were \\nintroduced for response estimation and for structural con trol. Zhao et al.[10] applied a counter-propagation NN to locate damage in beams and frames. \\nKuZniar and Waszczyszyn [11] simulated the dynamic response for prefabricated \\nbuilding using ANN. Elkordy et al.[12] used a back-propa gation neural network \\nwith modal shapes in the input layer  to detect the sim ulated damage of \\nstruc']\n"
          ]
        }
      ],
      "source": [
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "print(retrieved_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming retrieved_chunk is your list of text chunks corresponding to the nearest neighbors\n",
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "\n",
        "# Creating a DataFrame\n",
        "df = pd.DataFrame(retrieved_chunk, columns=['Retrieved Chunks'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo6KzPK2UyDg",
        "outputId": "494b7148-638e-41a7-dcd3-4c0793691edc"
      },
      "id": "Xo6KzPK2UyDg",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Retrieved Chunks\n",
            "0  tures. Muhammad [13] gives certain ANN applica...\n",
            "1  d esign of the building. All \\nbuildings have ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set option to increase width and max_colwidth\n",
        "pd.set_option('display.max_colwidth', None)  # For pandas versions >= 1.0\n",
        "pd.set_option('display.width', 1000)  # Adjust as necessary for your display\n",
        "\n",
        "df_full_text = pd.DataFrame([full_text], columns=['Full Text'])\n",
        "print(df_full_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Ji0wxpVRIW",
        "outputId": "d2bea557-f604-4a0f-9f16-0ce6c8ef580b"
      },
      "id": "d0Ji0wxpVRIW",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Full Text\n",
            "0  Response Prediction of Structural System Subject to  Earthquake \\nMotions using Artificial Neural Network \\n \\nS. Chakraverty*,  T. Marwala** , Pallavi Gupta* and  Thando Tettey**  \\n \\n*B.P.P.P. Division, Central Building Research Institu te \\nRoorkee-247 667, Uttaranchal, India \\ne-mail :sne_chak@yahoo.com \\n \\n** School of Electrical and Information Engineering, \\nUniversity of the Witwatersrand, Private Bag 3 \\nWits, 2050,Republic of South Africa  \\n  \\nAbstract \\nThis paper uses Artificial Neural Network (ANN) models to compute response of \\nstructural system subject to Indian earthquakes at Chamoli and Uttarkashi \\nground motion data. The system is first trained for a si ngle real earthquake data. \\nThe trained ANN architecture is then used to simulate e arthquakes with various \\nintensities and it was found that the predicted responses given by ANN model \\nare accurate for practical purposes. When the ANN is trai ned by a part of the \\nground motion data, it can also identify the responses of the structural system \\nwell. In this way the safeness of the structural systems ma y be predicted in case \\nof future earthquakes without waiting for the earthq uake to occur for the lessons. \\nTime period and the corresponding maximum response of t he building for an \\nearthquake has been evaluated, which is again trained to predict the maximum \\nresponse of the building at different time periods. T he trained time period versus \\nmaximum response ANN model is also tested for real earth quake data of other \\nplace, which was not used in the training and was found to be in good \\nagreement.  \\n \\nKeywords : Earthquake, Neural Network, Frequency, Structure, Buil ding. \\n  \\n \\n \\n1 Introduction \\n \\nReal earthquake ground motion at a particular build ing site is very complicated. \\nThe response of a building to an earthquake is dynamic and for a dynamic \\nresponse, the building is subjected to a vibratory shakin g of the base. Exactly \\nhow a building responds is complex and depends on the am plitude and \\nfrequency of vibration along with the material and d esign of the building. All \\nbuildings have a \"natural frequency\" associated with the m. If strain is placed on \\nto the structure and then let it snap back into equilibr ium, it will sway back and \\nforth with an amplitude that decays with time. If the  ground shakes with the same \\nfrequency as a building's natural frequency, it will cau se the amplitude of sway to \\nget larger and larger such that, the ground shaking is i n resonance with the \\nbuilding's natural frequency. This produces the most stra in on the components of \\nthe building and can quickly cause the building to colla pse. Powerful technique of \\nArtificial Neural Network (ANN) has been used to model the problem for one \\nstorey structure. Among the different types of ANN, the  feedforward, multilayer, \\nsupervised neural network with error back propagation al gorithm, the BPN [1] is \\nthe most frequently applied NN model. Dynamic response of a structure to strong \\nearthquake ground motion may be investigated by diff erent methods. The \\nmethod, that has been used here, is to create a trained  black box containing the \\ncharacteristics of the structure and of the earthquake moti on which can predict \\nthe dynamic response for any other earthquake for a pa rticular structure.  \\n \\nArtificial Neural Network (ANN) have gradually been es tablished as a powerful \\nsoft computing tool in various fields because of their excellent learning capacity \\nand their high tolerance to partially inaccurate data.  ANN has, recently been \\napplied to assess damage in structures. Stefano et al.[3]  used probabilistic \\nNeural Networks for seismic damage prediction. Many meth ods viz. [4]-[9] were \\nintroduced for response estimation and for structural con trol. Zhao et al.[10] applied a counter-propagation NN to locate damage in beams and frames. \\nKuZniar and Waszczyszyn [11] simulated the dynamic response for prefabricated \\nbuilding using ANN. Elkordy et al.[12] used a back-propa gation neural network \\nwith modal shapes in the input layer  to detect the sim ulated damage of \\nstructures. Muhammad [13] gives certain ANN applications in concrete \\nstructures. Pandey and Barai [14] detected damage in a bridge truss by applying \\nANN of multilayer perceptron architectures to numericall y simulated data. Some \\nstudies such as [15]-[17] used artificial neural network f or structural damage \\ndetection and system identification.  \\n \\nIn the present paper, the Chamoli earthquake ground acceleration at Barkot (NE) \\nand Uttarkashi earthquake ground acceleration recorded a t Barkot (NE and NW)  \\nhave been considered based on the authors' previous stud y [18]. From their \\nground acceleration the responses are computed using the u sual procedure. \\nThen the ground acceleration and the corresponding re sponse are trained using \\nArtificial Neural Network (ANN) with and without damp ing. After training the \\nnetwork with one earthquake, the converged weight mat rices are stored. In order \\nto show the power of these converged (trained) networ ks other earthquakes are \\nused as input to predict the direct response of the structu re without using any \\nmathematical analysis of the response prediction. Simi larly, the various time \\nperiods of one earthquake and its corresponding maximu m responses are \\ntrained. Then the converged weights are used to predict  the maximum response \\ndirectly to the corresponding time period. Various othe r results related to use of \\nthese trained networks are discussed for future / other e arthquakes. \\n \\n2 Artificial Neural Network  \\n \\nArtificial neural systems are present day machines that ha ve great potential to \\nimprove the quality of our life. Advances have been made in applying such \\nsystems for problems found difficult for traditional co mputation. A neural network \\nis a parallel, distributed information processing structur e consists of processing \\nelements called neurons, which are interconnected and u nidirectional signal channels called connections. The general structure of the network that have \\nbeen used here is given in Fig.1. the structure consists of  three layers : the input \\nlayer, the hidden layer and the output layer. The i nput layer is made up of one or \\nmore neurons or processing elements that collectively re present the information \\nin a particular pattern of a training set. The hidden  layer also consists of one or \\nmore neurons. Its purpose is simply to transform the info rmation from the input \\nlayer to prepare it for the output layer. The outpu t layer, which has one or more \\nneurons, uses input from the hidden layer (which is a tr ansformation of the input \\nlayer) to produce an output value for the entire ne twork. The output is used to \\ninterpret the training and classification results of the  network. The processing \\nelements or neurons are connected to each other by adju stable weights. The \\ninput/output behaviour of the network changes if the w eights are changed. So, \\nthe weights of the net may be chosen in such a way so as to achieve a desired \\noutput. To satisfy this goal, systematic ways of adjustin g the weights have to be \\ndeveloped, which are known as training or learning al gorithm.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Output Units \\nHidden units \\nInput Units  Z1 Zj Pj \\nP1 O1 Ok  \\n \\nFig.1. Layered Feedforward Neural Network \\n \\n3 Error Back Propagation Training Algorithm (EBPTA)   \\n \\nHere, Error Back Propagation Training algorithm and f eedforward recall with one \\nhidden layer has been used. In Fig. 1, Z i, P j and O k are input, hidden and output \\nlayer respectively. The weights between input and hidd en layers are denoted by \\nνji  and the weights between hidden and output layers are  denoted by W kj . The \\nprocedure may easily be written down for the processing of this algorithm. \\n \\nGiven R training pairs  \\nwhere Z i (Ix1) are input and d i (Kx1) are desired values for the given inputs, the \\nerror value is computed as \\nfor the present neural network as shown in Fig. 1.  \\n \\nThe error signal terms of the output ( δOk ) and hidden layers ( δPj ) are written \\nrespectively as, \\nConsequently, output layer weights (W kj ) and hidden layer weights ( δji ) are \\nadjusted as, \\n \\nWhere, β is the learning constant. \\n { }RRdZ dZdZ , ;......... ,;, 2211\\n( ) K k O d E k k ,..... 2 , 1 ,21 2= − =\\nK k O O d k k k Ok ,........ 2 , 1 ), 1)( ( *5 . 02= − − = δ\\n∑ = − =\\n=K\\nkPj Ok j Pj J j W P\\n12,.... 2 , 1 , ) 1 ( *5 . 0 δ δ\\nJ,..... 2 , 1j and K..... 2 , 1 k,P W Wj Ok )Old (\\nkj ) New (\\nkj = = + = βδ I ,....... 2 , 1 iand J....... 2 , 1j ,Zi Pj )Old (\\nji ) New (\\nji = = + = βδ ν ν \\n \\n \\n4 Response Prediction \\n \\nThe basic idea behind the proposed methodology is to pr edict the structural \\nresponse of single degree of freedom system i.e. single storey building subject to \\nvarious earthquake forces. Two cases viz without damping and with damping \\nhave been considered for the analysis. \\n \\nCase(i)  : Without damping \\n \\nLet M be the mass of the generalized one storey structur e, K the stiffness of the \\nstructure and x be the displacement relative to the gro und then the equation of \\nmotion may be written as: \\n \\nwhere, \\n \\nEquation (1) may be written as, \\n \\nWhere  ω2=K/M, is the natural frequency parameter of the undam ped structure. \\n \\nThe solution of equation (2) [Ref. 2] is given by \\nFrom this solution the response of the structure viz. acceleration is obtained for \\nno damping. \\n on. accelerati Ground ant, Displaceme xon, accelerati Response x\\n===\\n& &&&) 1 ( aM Kx xM & & & & −= +\\n) 2 (2a x x & & & & −= +ω\\n∫ − −=t\\n0) 3 ( d)] t ( sin[ )( a1) t ( x ττ ω τω& & \\n \\n \\nCase (ii) : With damping  \\n \\nLet M be the mass of the generalized one storey structur e, K the stiffness of the \\nstructure, C the damping and x be the displacement rela tive to the ground then \\nthe equation of motion may be written as: \\n \\nwhere \\n \\nEquation (4) may be written as, \\n \\nWhere ξω = C/2M and ω2=K/M, is the natural frequency parameter of the \\nundamped structure. \\nThe solution of equation (5) [Ref.2] is given by \\n \\nFrom this solution the response of the structure viz. acceleration is obtained for \\ndamping. \\n  \\nNow, the neural network architecture is constructed, takin g ground acceleration \\nas input and the response obtained from the above solu tion is taken as output for \\neach time step. Therefore, the whole network consists of o ne input layer, one \\nhidden layer with varying nodes and one output layer  as shown in Fig.1. Similarly ) 4 ( aM Kx xCxM & & && & −= + +\\non. accelerati Ground ant, Displaceme xvelocity, Response xon, accelerati Response x\\n====\\n& &&& &\\n) 5 ( 22a x x x & & && & −= + + ω ωξ\\n∫ − − − −=t\\n0) 6 ( d)] t ( sin[ )] t ( [exp )( a1) t ( x ττ ω τ ωξ τω& &for the other problem of time period vs. maximum resp onse the input and output \\nlayer contain the time period and the corresponding ma ximum response \\nrespectively at each interval for the particular structur e. \\n \\n5 Numerical Results and Discussions \\n \\nFor the present study two Indian earthquakes viz. the Chamoli Earthquake (max. \\nground acceleration =0.16885 m/sec/sec) at Barkot in NE (north–east) direction \\nshown in Fig.2(a) and the Uttarkashi earthquake  (maxi mum ground acceleration \\n= 0.931 m/sec/sec) at Barkot in NE (north–east) and NW ( north-west) direction \\nas given in Figs. 2(b) and 2(c) have been considered for  training and testing. \\n \\nInitially, the system without damping is studied and fo r that the ground \\nacceleration of Chamoli earthquake at Barkot (NE) was use d to compute the \\nresponse for single storey structure using usual procedure  from Eq.(3). The \\nobtained response and the ground acceleration is traine d first for the assumed \\nfrequency parameters ω=0.5 and ω=0.01 for time range 0 to 14.92 sec.(748 data \\npoints) for the mentioned earthquake. Simulations have  been done for different \\nhidden layer nodes and it was seen that the response re sult is almost same and \\ngood for 5 to 20 nodes in the hidden layer. However,  10 hidden layer nodes are \\nused here to generate further results. \\n \\nAfter training ground acceleration and response data fo r Chamoli earthquake at \\nBarkot (NE) for 10 nodes in hidden layer, the weights are stored and they are \\nused to predict responses for various intensity earthquakes.  The plot in Fig. 3(a) \\nshows response comparison between neural and desired for t he 80% of Chamoli \\nearthquake at Barkot (NE) for ω=0.01(maximum response=0.135079m/sec/sec). \\nSimilarly, the response comparison for 120% Chamoli ear thquake at Barkot (NE) \\nfor ω=0.5 (Maximum response=0.20260 m/sec/sec) is shown in Fig 3(b). \\n \\nNext, a part of the ground acceleration is used for the  training and it will be \\nshown that the present ANN can predict the whole perio d of the response using \\nthe trained ANN by the part of the data.  So, the g round acceleration and response data with Chamoli earthquake is trained for an example with the time \\nrange 0 to 10.96 sec.(550 data points). Its weights are stored to find the \\nresponse for the time range 0 to 14.92 sec. (whole per iod) at different \\npercentages of the earthquake in order to test the net work learning for the points \\noutside the training set. Figs. 4(a) and 4(b) show the response comparison \\nbetween neural and desired for ω=0.01, (maximum response=0.168849 \\nm/sec/sec) and for ω=0.5 (maximum response=0.168841 m/sec/sec) at the time \\nrange 0 to 10.96 sec. The response comparison between neural and desired for \\nω=0.01 with 120% of the earthquake force (maximum respo nse = 0.20260 \\nm/sec/sec) from the time range 0 to 14.92 sec.(748 data  points) is incorporated \\nin Fig. 5(a). It is obtained from the weights of the  trained data for the time range 0 \\nto 10.96 sec. (550 data points). From the same weights,  neural responses for \\n80% of earthquake force, is computed with ω=0.5(maximum response=0.135073 \\nm/sec/sec), for the time range 0 to 14.92 sec. (748 dat a points) and it is plotted in \\nFig. 5(b).  \\n \\nThe system with damping is then considered and for this,  first from the ground \\nacceleration of Chamoli Earthquake at Barkot (NE), the response is computed \\nusing Eq.(6). The obtained responses and the ground acc eleration are trained by \\nthe said ANN model for an example structural system wit h frequency parameter \\nω= 0.68981 and damping = 1.58033. This training was do ne for the total time \\nrange 0 to 14.92 sec. (748 points, earthquake period).  Plot of 100% response \\ncomparison between neural and desired for Chamoli Eart hquake at barkot (NE) \\nis shown in Fig. 6(a). After training ground accelerati on and response data for \\nChamoli Earthquake for various nodes in the hidden l ayer it was confirmed that \\n10 nodes are again sufficient for the prediction. So, the weights corresponding to \\n10 hidden nodes are stored and they are used to predict  responses for various \\nintensity earthquakes. The response for 50% ( ω= 0.68981, damping = 1.58033 \\nand maximum response = 0.00375 m/sec/sec) of the Chamol i Earthquake at \\nBarkot (NE) and its comparison with the desired response are shown in Fig. 6(b). \\nSimilarly, the response comparison between neural and desired is shown in Fig. 6(c) ( ω= 0.68981, damping = 1.58033 and maximum response fo r 120% = \\n0.00910 m/sec/sec) for 120% of earthquake acceleration. \\n \\nFinally, the Uttarkashi earthquake at Barkot (NW) gro und acceleration is used \\nwith damping = 0.05, at different time periods ( t =  1/omega) ranging from 0.5 to \\n10 with an interval of 0.02 (620 data points) for ev aluating the maximum \\nresponses corresponding to each time period using Eq. (6) . The obtained time \\nperiods and the corresponding responses are trained and t hen the converged \\nweights are stored. The comparison between neural and d esired results is shown \\nin Fig. 7(a). The stored weights were then used to pre dict the response for \\ndifferent time periods lying in the same range of 0. 5 to 10 but at different time \\ninterval of 0.5 for another earthquake such as Uttarkashi  earthquake at Tehri \\n(NW), The results are depicted in  Fig. 7(b) showing go od comparison between \\nANN model and desired results. \\n \\n6 Conclusions \\n \\nThis paper uses the powerful soft computing technique (Ar tificial Neural Network) \\nto compute structural response of single degree of freed om system subject to \\nIndian earthquakes at Chamoli and Uttarkashi ground mo tion data. Also this \\ntechnique is used to predict the maximum response corresp onding to various \\ntime periods. It is shown here that once the training is done then the trained \\narchitecture may be used to simulate for various intensit y earthquakes, thereby \\nshowing the responses of the system which depend upon the  structural \\nproperties (mass and stiffness) of the structure. If the ne twork is trained for \\nvarious time periods of one earthquake and its correspon ding maximum \\nresponses then the model can predict the maximum respons e directly to the \\ncorresponding time period for any other earthquake tha t had not been used \\nduring the training. In this way the safety of the str uctural systems may be \\npredicted in case of future earthquakes.  \\n \\nAcknowledgements The authors would like to thank Department of Science an d Technology, India for \\nfunding and Director C.B.R.I. for giving permission to  publish this paper.  \\n \\nReferences \\n[1] D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learn ing international \\nrepresentation by error propagation. In Parallel Dist ributed Processing, D.E. \\nRumelhart, et al. (eds). (The MIT Press: Cambridge, MA , 1986). \\n[2] N.M. Newmark, E. Rosenblueth, Fundamentals of Eart hquake Engineering, \\n(Prentice-Hall, Inc. Englewood Cliffs, N.J, 1971). \\n[3] A. De Stefano, D.Sabia, L.Sabia, Probabilistic Ne ural Networks for Seismic \\nDamage Mechanisms Prediction, Earthquake Engineering an d Structural \\nDynamics, Vol. 28, No. 8 (1999) 807- \\n[4] A. Kallassy, A new neural Network for Response Estima tion, Computers and \\nStructures, Vol. 81, No. 26-27 (2003), 2417-2429 \\n[5] A. Zhang, L. Zhang, RBF Neural Networks for the P rediction of Building \\nInterference effects, Computers and Structures, Vol. 82, No. 27(2004) 2333 \\n-2339 \\n[6] Brown.S. Aaron, T.Y. Henry, Yang, Neural Networ ks for multi Objective \\nAdaptive Structural Control, Structural Engineering A SCE, Vol.127, No.2, \\n(2001) 203- \\n[7] C.S. Huang, S.L. Hung, C.M. Wen, T.T.Tu, A Neur al Network Approach for \\nStructural Identification and Diagnosis of a Building f rom Seismic Response \\nData, Earthquake Engineering and Structural Dynamics, Vol. 32, No. 2, \\n(2003) 187-  \\n[8] C.Y. Kao, Shin Lin Hung, Detection of structural Damage via Free Vibration \\nResponses generated by Approximating Artificial Neural  Network, \\nComputers and Structures,Vol.81,No. 28-29 (2003), 2631 -2644 \\n[9] D.A. Liut, E.E Matheu, M.P. Singh, D.T. Mook, Ne ural Network Control of \\nbuilding Structures by a Force Matching Training Scheme,  Earthquake \\nEngineering and Structural Dynamics, Vol. 28, No. 12 (1999) 1601- [10] J .Zhao, J.N. Ivan, J.T. DeWolf, Structural damage  detection using artificial \\nneural networks. Journal of Infrastructure Systems (ASCE)  Vol.4, No.(3) \\n(1998) 93 – 101. \\n[11] Krystyna KuZniar, Zenon Waszczyszyn, Neural Simulat ion of Dynamic \\nResponse of Prefabricated Buildings Subjected to Parase ismic Excitations, \\nComputers and Structures,Vol. 81, No. 24-25 (2003) 235 3-2360 \\n[12] M.F. Elkordy, K.C. Chang, G.C. Lee, Neural netw orks trained by analytically \\nsimulated damage states. Journal of Computing in Civil Engineering (ASCE) \\nVol. 7, No.2 (1993) 130 – 145. \\n[13] N.S. Hadi Muhammad, Neural Networks Applications i n Concrete \\nStructures, Computers and Structures, Vol.81, No. 6 (20 03) 373-381 \\n[14] P.C. Pandey, S.V. Barai, Multilayer perceptron in damage detection of \\nbridge structures. Computers and Structures Vol. 54, No.4  (1995) 597- 608. \\n[15] Q. Chen, Y.W. Chan, K. Worden, Structural Fault  Diagnosis and Isolation \\nusing Neural Network Based on Response Only Data, Vol . 81, No. 22-23 \\n(2003) 2165 -2172 \\n[16] S.F. Masri, A.W. Smyth, A.G. Chassiakos, T.K. Caug hey, N.F. Hunter, \\nApplication of Neural networks for detection of changes i n nonlinear \\nsystems. Journal of Engineering Mechanics (ASCE), Vol. 126 , No.70 (2000) \\n666 – 676. \\n[17] S.L. Hung, C.Y. Kao, Structural Damage Detection  Using the Optimal \\nWeights of the Approximating Artificial Neural Networ ks, Earthquake \\nEngineering and Structural Dynamics,Vol. 31, No.2, (2 002) 217-  \\n[18] V.K. Mathur, S. Chakraverty and Pallavi Gupta,  Response Prediction of \\nTypical Rural House Subject to Earthquake Motions Using Artificial Neural \\nNetworks. Journal of Indian Building Congress, Vol.11, No.2 (2004) 99-105. \\n \\n \\n \\n     \\n         Fig. 2(a). Chamoli Earthquake at Barkot  in NE dire ction \\n                       Peak Acceleration = 0.16885m /sec/sec  \\n            \\nFig. 2(b) Uttarkashi Earthquake at Barkot in NE dir ection \\n       Peak Acceleration : 0.9317m/sec/sec  \\n                                \\n Fig. 2(c). Uttarkashi Earthquake at Barkot in NW d irection \\n             Peak Acceleration : 0.8470m/sec/sec  -15 -10 -5 0510 15 20 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration \\n-1 -0.8 -0.6 -0.4 -0.2 00.2 0.4 0.6 0.8 11.2 \\n0 5 10 15 20 25 30 35 \\nTime Acceleration \\n-1 -0.8 -0.6 -0.4 -0.2 00.2 0.4 0.6 0.8 1\\n0 5 10 15 20 25 30 35 \\nTime Acceleration              \\n \\nFig. 3(a). 80% Response comparison Between Neural a nd Desired of Chamoli \\n                         Earthquake at Barkot (NE) for ωω ωω=0.01  \\n \\nFig. 3(b). 120% Response comparison Between Neural and Desired of Chamoli \\n                         Earthquake at Barkot (NE) for ωω ωω=0.5  \\n -0.1 -0.05 00.05 0.1 0.15 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration Neural Desired \\n-0.15 -0.1 -0.05 00.05 0.1 0.15 0.2 0.25 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration Neural Desired                 \\nFig. 4(a). Response Comparison Between Neural and D esired (550 points) \\n                                   of Chamoli Earthquake at Barkot (NE)  for ωω ωω=0.01 \\n \\nFig. 4(b). Response Comparison Between Neural and D esired (550 points) \\n                            of Chamoli Earthquake at Barkot (NE)  for ωω ωω=0.5 \\n \\n -0.15 -0.1 -0.05 00.05 0.1 0.15 0.2 \\n0 2 4 6 8 10 12 \\nTime Acceleration Neural Desired \\n-0.15 -0.1 -0.05 00.05 0.1 0.15 0.2 \\n0 2 4 6 8 10 12 \\nTime Acceleration Neural Desired                 \\nFig. 5(a). 120% Response Comparison Between Neural and Desired \\n                of Chamoli Earthquake at Barkot (NE )  (748 points)   ωω ωω=0.01 \\n(After training from 550 points) \\n \\nFig. 5(b). 80% Response Comparison Between Neural a nd Desired \\n                                  of Chamoli Earthquake at Barkot (NE)  (748 points)   ωω ωω=0.5  \\n      (After training from 550 points) -0.15 -0.1 -0.05 00.05 0.1 0.15 0.2 0.25 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration Neural Desired \\n-0.1 -0.05 00.05 0.1 0.15 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration Neural Desired Fig.6(a). 100% Response Comparison Between Neural a nd Desired of Chamoli \\n                          Earthquake at Barkot (NE) with Damping \\nFig.6(b). 50% Response Comparison Between Neural an d Desired of Chamoli \\n                          Earthquake at Barkot (NE)  with Damping  \\nFig.6(c). 120% Response Comparison Between Neural a nd Desired of Chamoli \\n                          Earthquake at Barkot (NE)  with Damping  -0.006 -0.004 -0.002 00.002 0.004 0.006 0.008 0.01 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration Neural Desired \\n-0.003 -0.002 -0.001 00.001 0.002 0.003 0.004 0.005 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration Neural Desired \\n-0.008 -0.006 -0.004 -0.002 00.002 0.004 0.006 0.008 0.01 \\n0 2 4 6 8 10 12 14 16 \\nTime Acceleration Neural Desired                       \\nFig. 7(a). Comparison Between Neural and Desired fo r time period and the \\n                              corresponding maximum  response of Uttarkashi earthquake at           \\n                              Barkot in NW directio n ( 620 points ) \\n \\nFig. 7(b). Comparison Between Neural and Desired fo r time period and the    \\n             corresponding maximum response of Utta rkashi earthquake at \\n                              Tehri in NW direction  ( From weights of 620 points ) 00.001 0.002 0.003 0.004 0.005 0.006 0.007 \\n0 2 4 6 8 10 12 \\nTime Period Acceleration Neural Desired \\n00.001 0.002 0.003 0.004 0.005 0.006 0.007 \\n0 2 4 6 8 10 12 \\nTime period Acceleration Neural Desired \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b417a59-021a-411d-a491-cb31815192cd",
      "metadata": {
        "id": "4b417a59-021a-411d-a491-cb31815192cd"
      },
      "source": [
        "\n",
        "  \n",
        "### Combine context and question in a prompt and generate response\n",
        "\n",
        "Finally, we can present the retrieved text chunks as the context information within the prompt. Here is a prompt template where we can include both the retrieved text and user question in the prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "da042a53-4564-4057-9a60-9b57dffff6a1",
      "metadata": {
        "id": "da042a53-4564-4057-9a60-9b57dffff6a1"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def run_mistral_pipeline(user_message):\n",
        "    # Initialize the pipeline with the text-generation model\n",
        "    pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "    # Generate a response\n",
        "    response = pipe(user_message, max_length=512)[0]['generated_text']\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "YYqjirnKXRrX"
      },
      "id": "YYqjirnKXRrX",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1c5c20aa-6673-4105-9c10-886a1e18da8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456,
          "referenced_widgets": [
            "a4717a53d03d4dbaa88e4b529ebc2139",
            "6c3ef1d2bd334c19a91711e6a88311d1",
            "8c402c3b7c424495acc00553c9f08255",
            "53d4f1924bb541899777f04adef85983",
            "728d40edf0b64a569e830b8a0c1e0f65",
            "d0828f96e05d444896e801c9c5188e2a",
            "40d0fbd7844945ecb79513c51179e676",
            "b1c2af5b23b64818b6d2c5ad3e2d01b1",
            "03df7196545f4fcea18e1189a96c2150",
            "61d73f41f1204f6eae8fa79cea4ad80e",
            "50abc807ef4f480c90e9b346737034a4"
          ]
        },
        "id": "1c5c20aa-6673-4105-9c10-886a1e18da8a",
        "outputId": "6d381ad4-b807-4659-bd4d-df390e9bdbe2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4717a53d03d4dbaa88e4b529ebc2139"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: How to write an abstract similar to the response prediction using ANN paper?\n",
            "\n",
            "I'm trying to write an abstract for a paper on a neural network that predicts the response of a system to a given input. The paper is based on a deep learning model, specifically a feedforward neural network. The abstract should be similar to the response prediction using ANN paper by LeCun et al. (1990). Here's a draft of the abstract:\n",
            "\n",
            "---\n",
            "\n",
            "Title: Predicting System Responses with a Feedforward Neural Network\n",
            "\n",
            "Abstract: In this paper, we present a novel approach for predicting the response of a complex system to a given input using a feedforward neural network. Our model is inspired by the seminal work of LeCun et al. (1990) on recognizing handwritten digits using a backpropagation neural network. We apply this concept to predict the response of a system, which can be any physical or mathematical model, given a specific input.\n",
            "\n",
            "Our neural network consists of an input layer, multiple hidden layers, and an output layer. The input layer receives the system input, and the output layer produces the predicted response. The hidden layers contain a large number of neurons, each with a non-linear activation function. We use a sigmoid function as the activation function for the hidden neurons and a linear function for the output neurons.\n",
            "\n",
            "We train the neural network using a large dataset of input-response pairs. We use backpropagation with stochastic gradient descent to optimize the network's weights and biases. We also employ techniques such as dropout and batch normalization to prevent overfitting and improve generalization.\n",
            "\n",
            "We evaluate the performance of our model on several benchmark datasets, including the Mackey-Glass chaotic time series and the Lorenz system. Our results show that our neural network can accurately predict the response of these systems to various inputs, outperforming traditional methods such as analytical solutions and numerical simulations.\n",
            "\n",
            "Our work demonstrates the potential of feedforward neural networks as powerful tools for predicting the response of complex systems. This approach can be applied to various fields, including physics, engineering, and finance, to make accurate predictions and improve decision-making processes.\n",
            "\n",
            "---\n",
            "\n",
            "I would appreciate any feedback on the abstract, especially if it is similar to the response prediction using ANN paper by LeCun et al.\n"
          ]
        }
      ],
      "source": [
        "user_message = \"How to write an abstract similar to the response prediction using ANN paper?\"\n",
        "response = run_mistral_pipeline(user_message)\n",
        "print(\"Response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "possible_questions = [\n",
        "    [\n",
        "        \"What are the key keywords mentioned in the paper?\",\n",
        "        \"Can you list the main topics or subject areas covered in the paper?\",\n",
        "        \"What technical terms or concepts are referenced in the paper's keywords?\",\n",
        "        \"Are there any notable keywords missing that you would expect to see in a paper on this topic?\",\n",
        "        \"How do the keywords indicate the focus of the research presented in the paper?\"\n",
        "    ],\n",
        "    [\n",
        "        \"What is the main purpose of using Artificial Neural Networks in this research?\",\n",
        "        \"How are the ANN models trained and used to predict structural responses?\",\n",
        "        \"What are the key capabilities of the ANN models described in the paper?\",\n",
        "        \"How can the trained ANN models be used to assess the safety of structural systems?\",\n",
        "        \"What are the benefits of using ANN-based approaches for earthquake response prediction?\"\n",
        "    ],\n",
        "    [\n",
        "        \"How does a building's natural frequency affect its response to an earthquake?\",\n",
        "        \"What happens when the ground shaking frequency is in resonance with the building's natural frequency?\",\n",
        "        \"What factors determine a building's natural frequency?\",\n",
        "        \"Why can resonance between ground shaking and building frequency lead to increased risk of damage or collapse?\",\n",
        "        \"What is the relationship between a building's natural frequency and its response to earthquake motions?\"\n",
        "    ],\n",
        "    [\n",
        "        \"What type of neural network model is most commonly used for modeling the dynamic response of structures?\",\n",
        "        \"Can you describe the Back-Propagation Neural Network (BPN) and its application in this context?\",\n",
        "        \"Why is the BPN model particularly well-suited for modeling the dynamic response of structures?\",\n",
        "        \"Are there any other neural network architectures that have been explored for this application?\",\n",
        "        \"What are the key advantages of the BPN model for predicting structural responses to earthquakes?\"\n",
        "    ],\n",
        "    [\n",
        "        \"What data was used to train the ANN model for predicting earthquake responses?\",\n",
        "        \"How was the ANN model trained and validated for different earthquake intensities?\",\n",
        "        \"Can you describe the process of using the trained ANN architecture to predict structural responses over time?\",\n",
        "        \"What were the key findings regarding the accuracy of the ANN model's predictions?\",\n",
        "        \"Why is the Artificial Neural Network considered a powerful soft computing technique for this application?\"\n",
        "    ],\n",
        "    [\n",
        "        \"What are the key implications of the study's findings on using ANN models for predicting structural safety?\",\n",
        "        \"How does the training of the ANN model on Indian earthquake data contribute to its potential for accurate predictions?\",\n",
        "        \"What are the benefits of being able to predict the safeness of structural systems in advance of an earthquake?\",\n",
        "        \"How can the trained ANN model be used to simulate different earthquake intensities and study structural behavior?\",\n",
        "        \"Why do the study's findings represent a promising approach for ensuring the safety of buildings and structures during earthquakes?\"\n",
        "    ]\n",
        "]"
      ],
      "metadata": {
        "id": "BeYrvKyIQpNt"
      },
      "id": "BeYrvKyIQpNt",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Populating Lists and save as CSV File"
      ],
      "metadata": {
        "id": "Jfl7hohDCVBH"
      },
      "id": "Jfl7hohDCVBH"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text generation pipeline with the specified model\n",
        "def initialize_pipeline():\n",
        "    return pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "# Function to generate responses using the pipeline\n",
        "def run_mistral_pipeline(question, pipe):\n",
        "    # Generate response with specific settings including beam search and truncation\n",
        "    response = pipe(question, max_length=100, num_beams=5, length_penalty=2.0, truncation=True)[0]['generated_text']\n",
        "    return response\n",
        "\n",
        "# Main function to process questions and generate dataset\n",
        "def main():\n",
        "    # Initialize the pipeline\n",
        "    pipe = initialize_pipeline()\n",
        "\n",
        "    # List of questions to process\n",
        "    questions = [\n",
        "        \"What are the keywords mentioned in the paper titled Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network?\",\n",
        "        \"What is the purpose of using Artificial Neural Networks in earthquake response prediction?\",\n",
        "        \"How does the frequency of a building's natural frequency affect its response to an earthquake?\",\n",
        "        \"What kind of neural network model is most frequently applied for modeling dynamic response of structures?\",\n",
        "        \"How was the training of the ANN model conducted for predicting responses to various intensity earthquakes?\",\n",
        "        \"What is the significance of the study's findings on predicting the safeness of structural systems?\"\n",
        "    ]\n",
        "\n",
        "    # Corresponding ground truths for each question\n",
        "    ground_truths = [\n",
        "        \"The keywords written in the paper are: Earthquake, Neural Network, Frequency, Structure, Building.\",\n",
        "        \"Artificial Neural Networks (ANNs) are used to compute the response of structural systems to Indian earthquakes and simulate various intensities of earthquakes. The ANN model provides accurate predictions for practical purposes, allowing for the assessment of structural safety without the need for the earthquake to occur.\",\n",
        "        \"A building's response to an earthquake is dynamic and influenced by its natural frequency. If the ground shakes at the same frequency as the building's natural frequency, it causes resonance, leading to increased amplitude of sway and potential collapse due to the strain on building components.\",\n",
        "        \"The most frequently applied neural network model for modeling the dynamic response of structures is the feedforward, multilayer, supervised neural network with error backpropagation algorithm, known as the BPN.\",\n",
        "        \"The ANN model was trained using real earthquake data from the Chamoli and Uttarkashi earthquakes. The training involved using ground motion data to compute structural responses, which were then used to adjust the weights of the ANN for accurate future predictions.\",\n",
        "        \"The study's findings demonstrate the ability of the trained ANN architecture to simulate and predict the response of a structural system to future earthquakes. This can be crucial in predicting the safety of structures and in taking pre-emptive measures to mitigate earthquake damage.\"\n",
        "    ]\n",
        "\n",
        "    # Generate responses for each question using the initialized pipeline\n",
        "    rag_answers = [run_mistral_pipeline(question, pipe) for question in questions]\n",
        "\n",
        "    # Contexts and possible_questions arrays (placeholders for now)\n",
        "    contexts = ['']*len(questions)  # Empty context for each question\n",
        "    possible_questions = [[] for _ in questions]\n",
        "\n",
        "    # Compile data into a DataFrame\n",
        "    data = {\n",
        "        \"question\": questions,\n",
        "        \"ground_truth\": ground_truths,\n",
        "        \"rag_answer\": rag_answers,\n",
        "        \"context\": contexts,\n",
        "        \"possible_questions\": possible_questions\n",
        "    }\n",
        "    dataset = pd.DataFrame(data)\n",
        "\n",
        "    # Print and save the DataFrame to a CSV file\n",
        "    print(dataset)\n",
        "    dataset.to_csv('mistral_output.csv', index=False)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422,
          "referenced_widgets": [
            "dc120d0c6a4c486c887a0f012415d9a9",
            "630d19c321de497aa39f1b494aa52cc6",
            "8f9ffc786d4a406b9a1e67a01067a42b",
            "5b5f1816c1874c1e9d96382c07171880",
            "c3d7636c105e4380be68cf9eee5915d2",
            "5048176528bc44068bc84281b163243d",
            "9229a8a6ff50420db1a7b10d8f5a7984",
            "0545475e06b54d779c829ba5e8114096",
            "2dbad9a2271c4c87967253d82330513f",
            "f81b59442beb4a6bb0c3a86c3c535c9d",
            "d0dfe0dbda6d4937a700b6e6c7b3af2d"
          ]
        },
        "id": "RLzaUp0gCMOF",
        "outputId": "1e6f8a3f-481a-41fd-9906-faba8c760e0c"
      },
      "id": "RLzaUp0gCMOF",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc120d0c6a4c486c887a0f012415d9a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                      question                                                                                                                                                                                                                                                                                                                        ground_truth  \\\n",
            "0  What are the keywords mentioned in the paper titled Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network?                                                                                                                                                                                                                                  The keywords written in the paper are: Earthquake, Neural Network, Frequency, Structure, Building.   \n",
            "1                                                                   What is the purpose of using Artificial Neural Networks in earthquake response prediction?  Artificial Neural Networks (ANNs) are used to compute the response of structural systems to Indian earthquakes and simulate various intensities of earthquakes. The ANN model provides accurate predictions for practical purposes, allowing for the assessment of structural safety without the need for the earthquake to occur.   \n",
            "2                                                               How does the frequency of a building's natural frequency affect its response to an earthquake?                             A building's response to an earthquake is dynamic and influenced by its natural frequency. If the ground shakes at the same frequency as the building's natural frequency, it causes resonance, leading to increased amplitude of sway and potential collapse due to the strain on building components.   \n",
            "3                                                    What kind of neural network model is most frequently applied for modeling dynamic response of structures?                                                                                                                  The most frequently applied neural network model for modeling the dynamic response of structures is the feedforward, multilayer, supervised neural network with error backpropagation algorithm, known as the BPN.   \n",
            "4                                                   How was the training of the ANN model conducted for predicting responses to various intensity earthquakes?                                                            The ANN model was trained using real earthquake data from the Chamoli and Uttarkashi earthquakes. The training involved using ground motion data to compute structural responses, which were then used to adjust the weights of the ANN for accurate future predictions.   \n",
            "5                                                           What is the significance of the study's findings on predicting the safeness of structural systems?                                        The study's findings demonstrate the ability of the trained ANN architecture to simulate and predict the response of a structural system to future earthquakes. This can be crucial in predicting the safety of structures and in taking pre-emptive measures to mitigate earthquake damage.   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rag_answer context possible_questions  \n",
            "0                                                                                                                        What are the keywords mentioned in the paper titled Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network?\\n\\nThe keywords mentioned in the paper titled \"Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network\" are:\\n\\n1. Artificial Neural Network (ANN)\\n2. Earthquake motions\\n3. Structural system\\n4. Response prediction\\n5. Neural                         []  \n",
            "1                                                 What is the purpose of using Artificial Neural Networks in earthquake response prediction?\\nAnswer: Artificial Neural Networks (ANNs) are used in earthquake response prediction due to their ability to learn complex patterns and relationships from historical data. They can be trained on seismic data, such as ground motion records and structural response data, to predict the expected response of a structure during an earthquake. ANNs can also be used to identify the source parameters of an earthqu                         []  \n",
            "2                                            How does the frequency of a building's natural frequency affect its response to an earthquake?\\n\\nThe natural frequency of a building refers to the frequency at which it naturally oscillates when not subjected to external forces. When an earthquake occurs, the ground motion can excite the building's natural modes of vibration. If the natural frequency of the building is close to the frequency of the ground motion, then the building is more likely to resonate and experience larger amplitudes of motion                         []  \n",
            "3                                         What kind of neural network model is most frequently applied for modeling dynamic response of structures?\\n\\nAnswer:\\n\\nRecurrent Neural Networks (RNNs) are the most frequently applied neural network models for modeling dynamic response of structures. RNNs are a type of neural network that can process sequential data, making them well-suited for modeling time-series data such as structural response. They have a feedback connection that allows information from previous time steps to influence the current                         []  \n",
            "4                                                                        How was the training of the ANN model conducted for predicting responses to various intensity earthquakes?\\n\\nThe training of the ANN model for predicting responses to various intensity earthquakes was conducted using the backpropagation algorithm with the Levenberg-Marquardt optimization method. The model was trained on a dataset consisting of 10,000 input-output pairs, where the inputs were the ground motion parameters (peak ground acceleration, peak ground velocity, and                         []  \n",
            "5  What is the significance of the study's findings on predicting the safeness of structural systems?\\n\\nThe significance of the study's findings on predicting the safeness of structural systems lies in the fact that it provides a methodology for assessing the safety of structures based on their inherent properties and the environmental conditions they are subjected to. The study highlights the importance of considering both the material properties and the loading conditions when evaluating the safety of structures. The proposed methodology can                         []  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yndJjOoccSmG",
        "outputId": "54ed98c6-410e-4117-9e45-9db62220e59d"
      },
      "id": "yndJjOoccSmG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m143.4/163.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "# Initialize embedding model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load or initialize history\n",
        "def load_history():\n",
        "    if os.path.exists('session_history.csv'):\n",
        "        return pd.read_csv('session_history.csv')\n",
        "    else:\n",
        "        return pd.DataFrame(columns=['question', 'answer', 'embedding'])\n",
        "\n",
        "# Save history\n",
        "def save_history(history_df):\n",
        "    history_df.to_csv('session_history.csv', index=False)\n",
        "\n",
        "# Update session history with new Q&A and embeddings\n",
        "def update_history(question, answer, history_df):\n",
        "    embedding = embedder.encode(answer, convert_to_tensor=True)\n",
        "    new_data = pd.DataFrame({\n",
        "        'question': [question],\n",
        "        'answer': [answer],\n",
        "        'embedding': [embedding.numpy().tolist()]  # Ensure the embedding is properly serialized\n",
        "    })\n",
        "    history_df = pd.concat([history_df, new_data], ignore_index=True)\n",
        "    save_history(history_df)\n",
        "    return history_df\n",
        "\n",
        "# Generate response and update history\n",
        "def generate_response_and_update_history(question, history_df):\n",
        "    answer = pipe(question, max_length=100, num_beams=5, length_penalty=2.0, truncation=True)[0]['generated_text']\n",
        "    history_df = update_history(question, answer, history_df)\n",
        "    return answer, history_df\n",
        "\n",
        "# Main function to process questions\n",
        "def main():\n",
        "    history_df = load_history()\n",
        "\n",
        "    # Example questions\n",
        "    questions = [\n",
        "        \"How can the trained ANN model be used to simulate different earthquake intensities and study structural behavior?\",\n",
        "        \"Why do the study's findings represent a promising approach for ensuring the safety of buildings and structures during earthquakes?\"\n",
        "    ]\n",
        "\n",
        "    for question in questions:\n",
        "        answer, history_df = generate_response_and_update_history(question, history_df)\n",
        "        print(f\"Question: {question}\\nAnswer: {answer}\\n\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "6808c55b12794dbe8914c7063d96d995",
            "bd5527030b934586809eeee500db7094",
            "312eec0d9c3e42679d84053b76eb489b",
            "eff3fc9918f143a3a971810e2135e55d",
            "eaea3c08608141e787607b8962392ea4",
            "f299e2468eed4c0492ab465e101aad06",
            "2207424329c94da299d41dcf691bd6e6",
            "9ee42607c8954a56b6384b113567de62",
            "4b3ae5de627f4abf934b203eb05c0414",
            "76c7e3a0e0d1468890d14028706a7d86",
            "ea5f8609246245a6945e5cf9285a8ed3"
          ]
        },
        "id": "MBrVrt70X4K0",
        "outputId": "8ab0c7aa-a82c-45b4-c3b1-f0f9b3f0ed55"
      },
      "id": "MBrVrt70X4K0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6808c55b12794dbe8914c7063d96d995"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How can the trained ANN model be used to simulate different earthquake intensities and study structural behavior?\n",
            "Answer: How can the trained ANN model be used to simulate different earthquake intensities and study structural behavior?\n",
            "\n",
            "To simulate different earthquake intensities and study structural behavior, the trained ANN model can be used as follows:\n",
            "\n",
            "1. Define the input parameters: The input parameters to the ANN model are the ground motion parameters such as PGA, PGV, and spectral acceleration at different frequencies.\n",
            "2. Define the output parameters: The output parameters of the AN\n",
            "\n",
            "Question: Why do the study's findings represent a promising approach for ensuring the safety of buildings and structures during earthquakes?\n",
            "Answer: Why do the study's findings represent a promising approach for ensuring the safety of buildings and structures during earthquakes?\n",
            "\n",
            "The study's findings represent a promising approach for ensuring the safety of buildings and structures during earthquakes because they provide a new way to assess the seismic performance of structures based on their actual behavior during earthquakes, rather than relying solely on theoretical calculations or laboratory tests. This approach can help identify weaknesses and vulnerabilities in structures that may not be apparent through traditional\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:light"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ec5e8da894e441da7f5d4571247a454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79d3cbc6539247469b1f8123d69fcc46",
              "IPY_MODEL_c65087462b1f4266b01553b9d97b10b1",
              "IPY_MODEL_03ddf48c96dc4aa39dd08bc57baeff84"
            ],
            "layout": "IPY_MODEL_bd4df7400d7042f08dce74afe7bf0794"
          }
        },
        "79d3cbc6539247469b1f8123d69fcc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56de2393a5634d67a4aaf59643ce0789",
            "placeholder": "​",
            "style": "IPY_MODEL_ff0a419a0d094129b750947f4d2e0229",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c65087462b1f4266b01553b9d97b10b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_138b5dc066944f03af93a6f396ab1e35",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8be2a5ee0e314ba48cc07882696c0374",
            "value": 2
          }
        },
        "03ddf48c96dc4aa39dd08bc57baeff84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f965ccbf6a8414ea16b3a31ebe7aede",
            "placeholder": "​",
            "style": "IPY_MODEL_6daa5eee8f974d26982b671b1ee3c11e",
            "value": " 2/2 [00:51&lt;00:00, 24.03s/it]"
          }
        },
        "bd4df7400d7042f08dce74afe7bf0794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56de2393a5634d67a4aaf59643ce0789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff0a419a0d094129b750947f4d2e0229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "138b5dc066944f03af93a6f396ab1e35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8be2a5ee0e314ba48cc07882696c0374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f965ccbf6a8414ea16b3a31ebe7aede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6daa5eee8f974d26982b671b1ee3c11e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4717a53d03d4dbaa88e4b529ebc2139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c3ef1d2bd334c19a91711e6a88311d1",
              "IPY_MODEL_8c402c3b7c424495acc00553c9f08255",
              "IPY_MODEL_53d4f1924bb541899777f04adef85983"
            ],
            "layout": "IPY_MODEL_728d40edf0b64a569e830b8a0c1e0f65"
          }
        },
        "6c3ef1d2bd334c19a91711e6a88311d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0828f96e05d444896e801c9c5188e2a",
            "placeholder": "​",
            "style": "IPY_MODEL_40d0fbd7844945ecb79513c51179e676",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8c402c3b7c424495acc00553c9f08255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1c2af5b23b64818b6d2c5ad3e2d01b1",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03df7196545f4fcea18e1189a96c2150",
            "value": 3
          }
        },
        "53d4f1924bb541899777f04adef85983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61d73f41f1204f6eae8fa79cea4ad80e",
            "placeholder": "​",
            "style": "IPY_MODEL_50abc807ef4f480c90e9b346737034a4",
            "value": " 3/3 [00:54&lt;00:00, 17.62s/it]"
          }
        },
        "728d40edf0b64a569e830b8a0c1e0f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0828f96e05d444896e801c9c5188e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40d0fbd7844945ecb79513c51179e676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1c2af5b23b64818b6d2c5ad3e2d01b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03df7196545f4fcea18e1189a96c2150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61d73f41f1204f6eae8fa79cea4ad80e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50abc807ef4f480c90e9b346737034a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc120d0c6a4c486c887a0f012415d9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_630d19c321de497aa39f1b494aa52cc6",
              "IPY_MODEL_8f9ffc786d4a406b9a1e67a01067a42b",
              "IPY_MODEL_5b5f1816c1874c1e9d96382c07171880"
            ],
            "layout": "IPY_MODEL_c3d7636c105e4380be68cf9eee5915d2"
          }
        },
        "630d19c321de497aa39f1b494aa52cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5048176528bc44068bc84281b163243d",
            "placeholder": "​",
            "style": "IPY_MODEL_9229a8a6ff50420db1a7b10d8f5a7984",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8f9ffc786d4a406b9a1e67a01067a42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0545475e06b54d779c829ba5e8114096",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dbad9a2271c4c87967253d82330513f",
            "value": 3
          }
        },
        "5b5f1816c1874c1e9d96382c07171880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81b59442beb4a6bb0c3a86c3c535c9d",
            "placeholder": "​",
            "style": "IPY_MODEL_d0dfe0dbda6d4937a700b6e6c7b3af2d",
            "value": " 3/3 [00:45&lt;00:00, 15.04s/it]"
          }
        },
        "c3d7636c105e4380be68cf9eee5915d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5048176528bc44068bc84281b163243d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9229a8a6ff50420db1a7b10d8f5a7984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0545475e06b54d779c829ba5e8114096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dbad9a2271c4c87967253d82330513f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f81b59442beb4a6bb0c3a86c3c535c9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0dfe0dbda6d4937a700b6e6c7b3af2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6808c55b12794dbe8914c7063d96d995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd5527030b934586809eeee500db7094",
              "IPY_MODEL_312eec0d9c3e42679d84053b76eb489b",
              "IPY_MODEL_eff3fc9918f143a3a971810e2135e55d"
            ],
            "layout": "IPY_MODEL_eaea3c08608141e787607b8962392ea4"
          }
        },
        "bd5527030b934586809eeee500db7094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f299e2468eed4c0492ab465e101aad06",
            "placeholder": "​",
            "style": "IPY_MODEL_2207424329c94da299d41dcf691bd6e6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "312eec0d9c3e42679d84053b76eb489b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ee42607c8954a56b6384b113567de62",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b3ae5de627f4abf934b203eb05c0414",
            "value": 3
          }
        },
        "eff3fc9918f143a3a971810e2135e55d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76c7e3a0e0d1468890d14028706a7d86",
            "placeholder": "​",
            "style": "IPY_MODEL_ea5f8609246245a6945e5cf9285a8ed3",
            "value": " 3/3 [01:05&lt;00:00, 21.26s/it]"
          }
        },
        "eaea3c08608141e787607b8962392ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f299e2468eed4c0492ab465e101aad06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2207424329c94da299d41dcf691bd6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ee42607c8954a56b6384b113567de62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3ae5de627f4abf934b203eb05c0414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76c7e3a0e0d1468890d14028706a7d86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea5f8609246245a6945e5cf9285a8ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}